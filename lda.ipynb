{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from random import Random\n",
    "from texch.experiments import ClusteringExperiment, MultiClusteringExperiment\n",
    "from texch.clustering.nltk import KMeansClusterer\n",
    "from texch.preprocessing import PreprocessStep, Preprocessor\n",
    "from texch.preprocessing.sklearn import TfidfVectorizer\n",
    "from texch.clustering.nltk import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    categories=[\n",
    "        'alt.atheism', 'talk.religion.misc',\n",
    "        'comp.graphics', 'sci.space'\n",
    "    ],\n",
    "    subset='test',\n",
    "    random_state=42\n",
    ")\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "data = []\n",
    "for text in dataset.data:\n",
    "    sent_tokens = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        sent = sentence.lower()\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        sent_tokens += [w for w in tokens if not w in stopset]\n",
    "    data.append(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast version of gensim.models.doc2vec is being used\n",
      "'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding document #0 to Dictionary(0 unique tokens: [])\n",
      "built Dictionary(27114 unique tokens: [u'askew', u'woods', u'hanging', u'fractioning', u'5980']...) from 1353 documents (total 276586 corpus positions)\n",
      "discarding 3 tokens: [(u'lines', 1351), (u'subject', 1353), (u'organization', 1315)]...\n",
      "keeping 27111 tokens which were in no less than 1 and no more than 1082 (=80.0%) documents\n",
      "rebuilding dictionary, shrinking gaps\n",
      "resulting dictionary: Dictionary(27111 unique tokens: [u'askew', u'woods', u'hanging', u'fractioning', u'5980']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using symmetric alpha at 0.5\n",
      "using symmetric eta at 3.68853970713e-05\n",
      "using serial LDA version on this node\n",
      "running online LDA training, 2 topics, 10 passes over the supplied corpus of 1353 documents, updating model once every 500 documents, evaluating perplexity every 1000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "PROGRESS: pass 0, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "0/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "1/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "0/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "0/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "0/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"com\" + 0.005*\"would\" + 0.003*\"graphics\" + 0.003*\"1\" + 0.003*\"image\" + 0.003*\"one\" + 0.003*\"like\" + 0.003*\"_\" + 0.003*\"writes\"\n",
      "topic #1 (0.500): 0.008*\"edu\" + 0.004*\"one\" + 0.004*\"image\" + 0.004*\"1\" + 0.004*\"god\" + 0.004*\"would\" + 0.003*\"writes\" + 0.003*\"com\" + 0.003*\"article\" + 0.002*\"also\"\n",
      "topic diff=1.706994, rho=1.000000\n",
      "PROGRESS: pass 0, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "10/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "7/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "8/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "10/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-9.409 per-word bound, 679.7 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 0, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "9/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"would\" + 0.005*\"com\" + 0.003*\"image\" + 0.003*\"1\" + 0.003*\"writes\" + 0.003*\"one\" + 0.003*\"like\" + 0.003*\"jpeg\" + 0.003*\"_\"\n",
      "topic #1 (0.500): 0.008*\"edu\" + 0.005*\"god\" + 0.005*\"one\" + 0.004*\"1\" + 0.004*\"would\" + 0.004*\"writes\" + 0.004*\"com\" + 0.003*\"article\" + 0.003*\"image\" + 0.002*\"also\"\n",
      "topic diff=0.622996, rho=0.408248\n",
      "PROGRESS: pass 0, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "21/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "17/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 0, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "21/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-9.145 per-word bound, 566.0 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 0, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "10/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"would\" + 0.005*\"com\" + 0.003*\"1\" + 0.003*\"writes\" + 0.003*\"like\" + 0.003*\"one\" + 0.003*\"space\" + 0.003*\"article\" + 0.003*\"image\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.005*\"god\" + 0.005*\"one\" + 0.004*\"writes\" + 0.004*\"would\" + 0.004*\"com\" + 0.004*\"1\" + 0.003*\"article\" + 0.002*\"say\" + 0.002*\"image\"\n",
      "topic diff=0.387502, rho=0.301511\n",
      "PROGRESS: pass 1, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "54/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "61/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "49/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "56/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "48/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"would\" + 0.005*\"com\" + 0.004*\"1\" + 0.004*\"image\" + 0.003*\"graphics\" + 0.003*\"writes\" + 0.003*\"one\" + 0.003*\"like\" + 0.003*\"space\"\n",
      "topic #1 (0.500): 0.008*\"edu\" + 0.005*\"god\" + 0.005*\"one\" + 0.004*\"would\" + 0.004*\"com\" + 0.004*\"writes\" + 0.004*\"1\" + 0.003*\"article\" + 0.003*\"say\" + 0.002*\"think\"\n",
      "topic diff=0.231995, rho=0.258199\n",
      "PROGRESS: pass 1, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "56/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "57/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "55/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "62/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.795 per-word bound, 444.2 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 1, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "57/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"would\" + 0.004*\"com\" + 0.004*\"image\" + 0.004*\"1\" + 0.003*\"jpeg\" + 0.003*\"space\" + 0.003*\"writes\" + 0.003*\"one\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.008*\"edu\" + 0.006*\"god\" + 0.005*\"one\" + 0.004*\"com\" + 0.004*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"1\" + 0.003*\"say\" + 0.003*\"think\"\n",
      "topic diff=0.250983, rho=0.258199\n",
      "PROGRESS: pass 1, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "65/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "73/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 1, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "55/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.867 per-word bound, 466.8 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 1, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "36/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"would\" + 0.004*\"com\" + 0.004*\"1\" + 0.004*\"space\" + 0.003*\"image\" + 0.003*\"writes\" + 0.003*\"like\" + 0.003*\"one\" + 0.003*\"graphics\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.005*\"one\" + 0.004*\"com\" + 0.004*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"1\" + 0.003*\"say\" + 0.003*\"think\"\n",
      "topic diff=0.224498, rho=0.258199\n",
      "PROGRESS: pass 2, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "68/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "69/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "62/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "58/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "65/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.004*\"would\" + 0.004*\"image\" + 0.004*\"com\" + 0.004*\"1\" + 0.003*\"space\" + 0.003*\"graphics\" + 0.003*\"writes\" + 0.003*\"jpeg\" + 0.003*\"one\"\n",
      "topic #1 (0.500): 0.008*\"edu\" + 0.006*\"god\" + 0.005*\"one\" + 0.005*\"com\" + 0.004*\"would\" + 0.004*\"writes\" + 0.004*\"article\" + 0.003*\"1\" + 0.003*\"say\" + 0.003*\"people\"\n",
      "topic diff=0.230665, rho=0.250000\n",
      "PROGRESS: pass 2, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "76/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "69/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "70/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "71/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.715 per-word bound, 420.3 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 2, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "75/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.004*\"would\" + 0.004*\"image\" + 0.004*\"1\" + 0.004*\"com\" + 0.004*\"space\" + 0.003*\"jpeg\" + 0.003*\"graphics\" + 0.003*\"writes\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.008*\"edu\" + 0.006*\"god\" + 0.005*\"one\" + 0.005*\"com\" + 0.004*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"1\" + 0.003*\"people\"\n",
      "topic diff=0.215713, rho=0.250000\n",
      "PROGRESS: pass 2, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "79/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "85/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 2, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "75/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.789 per-word bound, 442.2 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 2, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "40/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"would\" + 0.004*\"1\" + 0.004*\"com\" + 0.004*\"space\" + 0.004*\"image\" + 0.003*\"writes\" + 0.003*\"graphics\" + 0.003*\"like\" + 0.003*\"one\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.005*\"one\" + 0.005*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"1\"\n",
      "topic diff=0.190032, rho=0.250000\n",
      "PROGRESS: pass 3, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "73/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "84/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "77/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "80/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "75/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.004*\"image\" + 0.004*\"would\" + 0.004*\"1\" + 0.004*\"com\" + 0.004*\"space\" + 0.004*\"graphics\" + 0.003*\"jpeg\" + 0.003*\"writes\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.008*\"edu\" + 0.006*\"god\" + 0.005*\"one\" + 0.005*\"com\" + 0.004*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"1\"\n",
      "topic diff=0.214605, rho=0.242536\n",
      "PROGRESS: pass 3, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "74/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "82/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "78/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.680 per-word bound, 410.0 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 3, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "77/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.004*\"image\" + 0.004*\"1\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"com\" + 0.003*\"jpeg\" + 0.003*\"graphics\" + 0.003*\"writes\" + 0.003*\"_\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.005*\"one\" + 0.005*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.192360, rho=0.242536\n",
      "PROGRESS: pass 3, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "93/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "85/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 3, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "84/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.754 per-word bound, 431.7 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 3, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "43/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.004*\"would\" + 0.004*\"1\" + 0.004*\"space\" + 0.004*\"image\" + 0.004*\"com\" + 0.003*\"graphics\" + 0.003*\"writes\" + 0.003*\"like\" + 0.003*\"jpeg\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.005*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.169153, rho=0.242536\n",
      "PROGRESS: pass 4, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "84/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "84/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "82/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "82/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"image\" + 0.005*\"1\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"graphics\" + 0.004*\"com\" + 0.003*\"jpeg\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.005*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.199532, rho=0.235702\n",
      "PROGRESS: pass 4, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "80/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "84/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "84/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.659 per-word bound, 404.2 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 4, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "83/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"image\" + 0.005*\"1\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"jpeg\" + 0.003*\"com\" + 0.003*\"graphics\" + 0.003*\"writes\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.005*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.175766, rho=0.235702\n",
      "PROGRESS: pass 4, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 4, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "85/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.738 per-word bound, 427.0 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 4, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "45/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"image\" + 0.004*\"com\" + 0.003*\"graphics\" + 0.003*\"like\" + 0.003*\"writes\" + 0.003*\"jpeg\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.005*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.157028, rho=0.235702\n",
      "PROGRESS: pass 5, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "81/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "85/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"graphics\" + 0.003*\"com\" + 0.003*\"jpeg\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.005*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.188554, rho=0.229416\n",
      "PROGRESS: pass 5, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "85/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "82/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "93/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.647 per-word bound, 400.8 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 5, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "82/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"image\" + 0.005*\"1\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"jpeg\" + 0.003*\"graphics\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"writes\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.163912, rho=0.229416\n",
      "PROGRESS: pass 5, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "91/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 5, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.730 per-word bound, 424.6 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 5, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "46/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"image\" + 0.003*\"com\" + 0.003*\"graphics\" + 0.003*\"like\" + 0.003*\"2\" + 0.003*\"writes\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.149459, rho=0.229416\n",
      "PROGRESS: pass 6, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "84/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"graphics\" + 0.003*\"com\" + 0.003*\"jpeg\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"know\"\n",
      "topic diff=0.179685, rho=0.223607\n",
      "PROGRESS: pass 6, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "85/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "82/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "91/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.639 per-word bound, 398.5 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 6, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "81/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"jpeg\" + 0.003*\"graphics\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.155557, rho=0.223607\n",
      "PROGRESS: pass 6, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "91/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 6, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.725 per-word bound, 423.2 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 6, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "47/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"image\" + 0.003*\"com\" + 0.003*\"graphics\" + 0.003*\"2\" + 0.003*\"jpeg\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.144076, rho=0.223607\n",
      "PROGRESS: pass 7, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "90/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "84/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "90/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"graphics\" + 0.003*\"jpeg\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.004*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"know\"\n",
      "topic diff=0.172481, rho=0.218218\n",
      "PROGRESS: pass 7, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "83/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "93/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.632 per-word bound, 396.6 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 7, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"space\" + 0.004*\"would\" + 0.004*\"jpeg\" + 0.003*\"graphics\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.149029, rho=0.218218\n",
      "PROGRESS: pass 7, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "91/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 7, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.721 per-word bound, 421.9 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 7, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "48/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.004*\"space\" + 0.004*\"would\" + 0.004*\"image\" + 0.003*\"graphics\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"jpeg\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"com\" + 0.006*\"one\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.139596, rho=0.218218\n",
      "PROGRESS: pass 8, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "92/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"graphics\" + 0.003*\"jpeg\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"com\" + 0.006*\"one\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"people\" + 0.003*\"say\" + 0.003*\"know\"\n",
      "topic diff=0.166379, rho=0.213201\n",
      "PROGRESS: pass 8, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "86/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "93/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.627 per-word bound, 395.3 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 8, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"space\" + 0.004*\"would\" + 0.004*\"jpeg\" + 0.003*\"graphics\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"people\" + 0.003*\"say\" + 0.003*\"think\"\n",
      "topic diff=0.143718, rho=0.213201\n",
      "PROGRESS: pass 8, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "92/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 8, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.717 per-word bound, 420.7 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 8, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "49/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.004*\"space\" + 0.004*\"would\" + 0.004*\"image\" + 0.003*\"graphics\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"jpeg\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"com\" + 0.006*\"one\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.135603, rho=0.213201\n",
      "PROGRESS: pass 9, at document #100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "87/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #400/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "90/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #500/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "92/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"would\" + 0.004*\"space\" + 0.004*\"graphics\" + 0.003*\"jpeg\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"com\" + 0.006*\"one\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"people\" + 0.003*\"say\" + 0.003*\"think\"\n",
      "topic diff=0.161044, rho=0.208514\n",
      "PROGRESS: pass 9, at document #600/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #700/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #800/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "88/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #900/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "91/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.623 per-word bound, 394.3 perplexity estimate based on a held-out corpus of 100 documents with 21065 words\n",
      "PROGRESS: pass 9, at document #1000/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "89/100 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 500 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.005*\"image\" + 0.004*\"space\" + 0.004*\"would\" + 0.004*\"jpeg\" + 0.003*\"graphics\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"also\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"one\" + 0.006*\"com\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"people\" + 0.003*\"say\" + 0.003*\"think\"\n",
      "topic diff=0.139167, rho=0.208514\n",
      "PROGRESS: pass 9, at document #1100/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "93/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #1200/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "90/100 documents converged within 50 iterations\n",
      "PROGRESS: pass 9, at document #1300/1353\n",
      "performing inference on a chunk of 100 documents\n",
      "91/100 documents converged within 50 iterations\n",
      "bound: at document #0\n",
      "-8.713 per-word bound, 419.7 perplexity estimate based on a held-out corpus of 53 documents with 7774 words\n",
      "PROGRESS: pass 9, at document #1353/1353\n",
      "performing inference on a chunk of 53 documents\n",
      "49/53 documents converged within 50 iterations\n",
      "updating topics\n",
      "merging changes from 353 documents into a model of 1353 documents\n",
      "topic #0 (0.500): 0.011*\"edu\" + 0.005*\"1\" + 0.004*\"space\" + 0.004*\"would\" + 0.004*\"image\" + 0.003*\"graphics\" + 0.003*\"com\" + 0.003*\"2\" + 0.003*\"jpeg\" + 0.003*\"like\"\n",
      "topic #1 (0.500): 0.009*\"edu\" + 0.006*\"god\" + 0.006*\"com\" + 0.006*\"one\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"article\" + 0.003*\"say\" + 0.003*\"people\" + 0.003*\"think\"\n",
      "topic diff=0.131771, rho=0.208514\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus, num_topics=2, \n",
    "                            id2word=dictionary, \n",
    "                            update_every=5, \n",
    "                            chunksize=100, \n",
    "                            passes=10,\n",
    "                            random_state=15\n",
    "                      \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for text in corpus:\n",
    "    d = [0 for i in xrange(2)]\n",
    "    for topic, score in lda[text]:\n",
    "        d[topic] = score\n",
    "    data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from texch.clustering.sklearn import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ClusteringExperiment(\n",
    "    method=KMeans(\n",
    "        true_k,\n",
    "        random_state=15\n",
    "    ),\n",
    "    preprocessor=Preprocessor([PreprocessStep(lambda x: x)]),\n",
    "    verbose_name='stopwords removed unigrams'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.set_input_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment \"stopwords removed unigrams (id=16)\"...\n",
      "Running preprocessing...\n",
      "Step #0: PreprocessStep (id=16): finished in 8.10623168945e-06 sec\n",
      "Finished preprocessing in 8.10623168945e-06\n",
      "Running method...\n",
      "Finished method in 0.0985021591187 sec\n",
      "Finished experiment in 0.0985102653503 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <center>\n",
       "            <h1> Experiment Summary </h1><br>\n",
       "            </center>\n",
       "            Experiment name: <br><b>stopwords removed unigrams</b><br>\n",
       "            <ul>\n",
       "            <li>\n",
       "            <b>Preprocessor</b>: <br> <ul><li>PreprocessStep (id=16)</li></ul>\n",
       "            </li>\n",
       "            <li>\n",
       "            <b>Method</b>: <br>SklearnClusterer: \n",
       "            </li>\n",
       "            </ul>\n",
       "        <br><br>            Total objects to cluster: 1353<br><br>            Total clusters found: 4<br>        Cluster #0: 547 objects<br>Cluster #1: 622 objects<br>Cluster #2: 85 objects<br>Cluster #3: 99 objects<br><br><br>Computed scores:<br><br><div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExperimentID</th>\n",
       "      <th>ExperimentName</th>\n",
       "      <th>MethodSpent</th>\n",
       "      <th>PrepareFuncSpent</th>\n",
       "      <th>PreprocessorSpent</th>\n",
       "      <th>TotalSpent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>stopwords removed unigrams</td>\n",
       "      <td>0.098502</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.09851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\n",
       "            Total objects to cluster: 1353\n",
       "\n",
       "            Total clusters found: 4\n",
       "        Cluster #0: 547 objects\n",
       "Cluster #1: 622 objects\n",
       "Cluster #2: 85 objects\n",
       "Cluster #3: 99 objects\n",
       "\n",
       "\n",
       "Scores:\n",
       "\n",
       "   ExperimentID              ExperimentName  MethodSpent  PrepareFuncSpent  \\\n",
       "0            16  stopwords removed unigrams     0.098502                 0   \n",
       "\n",
       "   PreprocessorSpent  TotalSpent  \n",
       "0           0.000008     0.09851  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExperimentID</th>\n",
       "      <th>ExperimentName</th>\n",
       "      <th>MethodSpent</th>\n",
       "      <th>PrepareFuncSpent</th>\n",
       "      <th>PreprocessorSpent</th>\n",
       "      <th>TotalSpent</th>\n",
       "      <th>entropy</th>\n",
       "      <th>homogeneity</th>\n",
       "      <th>v_measure</th>\n",
       "      <th>adj_rand_index</th>\n",
       "      <th>completeness</th>\n",
       "      <th>mutual_info_score</th>\n",
       "      <th>normalized_mutual_info_score</th>\n",
       "      <th>adjusted_mutual_info_score</th>\n",
       "      <th>fowlkes_mallows_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>stopwords removed unigrams</td>\n",
       "      <td>0.098502</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.09851</td>\n",
       "      <td>1.088598</td>\n",
       "      <td>0.372445</td>\n",
       "      <td>0.415186</td>\n",
       "      <td>0.375778</td>\n",
       "      <td>0.469007</td>\n",
       "      <td>0.51056</td>\n",
       "      <td>0.417947</td>\n",
       "      <td>0.370908</td>\n",
       "      <td>0.579327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ExperimentID              ExperimentName  MethodSpent  PrepareFuncSpent  \\\n",
       "0            16  stopwords removed unigrams     0.098502                 0   \n",
       "\n",
       "   PreprocessorSpent  TotalSpent   entropy  homogeneity  v_measure  \\\n",
       "0           0.000008     0.09851  1.088598     0.372445   0.415186   \n",
       "\n",
       "   adj_rand_index  completeness  mutual_info_score  \\\n",
       "0        0.375778      0.469007            0.51056   \n",
       "\n",
       "   normalized_mutual_info_score  adjusted_mutual_info_score  \\\n",
       "0                      0.417947                    0.370908   \n",
       "\n",
       "   fowlkes_mallows_score  \n",
       "0               0.579327  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.set_true_labels(labels)\n",
    "c.compute_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCORES = [\n",
    "    'homogeneity', 'completeness', 'v_measure',\n",
    "    'adj_rand_index', 'adjusted_mutual_info_score',\n",
    "    'fowlkes_mallows_score',\n",
    "    'silhouette_coefficient', 'calinski_harabaz_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExperimentName</th>\n",
       "      <th>homogeneity</th>\n",
       "      <th>completeness</th>\n",
       "      <th>v_measure</th>\n",
       "      <th>adj_rand_index</th>\n",
       "      <th>adjusted_mutual_info_score</th>\n",
       "      <th>fowlkes_mallows_score</th>\n",
       "      <th>silhouette_coefficient</th>\n",
       "      <th>calinski_harabaz_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stopwords removed unigrams</td>\n",
       "      <td>0.372445</td>\n",
       "      <td>0.469007</td>\n",
       "      <td>0.415186</td>\n",
       "      <td>0.375778</td>\n",
       "      <td>0.370908</td>\n",
       "      <td>0.579327</td>\n",
       "      <td>0.846652</td>\n",
       "      <td>44786.672173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ExperimentName  homogeneity  completeness  v_measure  \\\n",
       "0  stopwords removed unigrams     0.372445      0.469007   0.415186   \n",
       "\n",
       "   adj_rand_index  adjusted_mutual_info_score  fowlkes_mallows_score  \\\n",
       "0        0.375778                    0.370908               0.579327   \n",
       "\n",
       "   silhouette_coefficient  calinski_harabaz_score  \n",
       "0                0.846652            44786.672173  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.compute_scores(['silhouette_coefficient', 'calinski_harabaz_score'])\n",
    "c.result[['ExperimentName'] + SCORES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
